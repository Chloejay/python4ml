{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---Raynard\n",
    "\n",
    "Based on paper [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)\n",
    "\n",
    "But with my own coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "\n",
    "Available datasets are: \n",
    "```\n",
    "apple2orange, \n",
    "summer2winter_yosemite, \n",
    "horse2zebra, \n",
    "monet2photo, \n",
    "cezanne2photo, \n",
    "ukiyoe2photo, \n",
    "vangogh2photo, \n",
    "maps, \n",
    "cityscapes, \n",
    "facades, \n",
    "iphone2dslr_flower, \n",
    "ae_photos\n",
    "```\n",
    "download link\n",
    "```\n",
    "https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/$FILE.zip\n",
    "```\n",
    "\n",
    "https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_rfpad(ni,no,ks,stride=1):\n",
    "    pad = ks//2\n",
    "    return nn.Sequential(*[\n",
    "        nn.ReflectionPad2d([pad]*4),\n",
    "        nn.Conv2d(ni,no,kernel_size = ks, stride = stride, padding=0, bias = False),\n",
    "        nn.InstanceNorm2d(no),\n",
    "        nn.LeakyReLU(inplace = True)\n",
    "    ])\n",
    "\n",
    "def conv_layer_ins(ni,no,ks,stride=1):\n",
    "    return nn.Sequential(*[\n",
    "        nn.Conv2d(ni,no,kernel_size = ks, stride = stride, padding = (1,1),bias = False),\n",
    "        nn.InstanceNorm2d(no),\n",
    "        nn.LeakyReLU(inplace = True)\n",
    "    ])\n",
    "\n",
    "def deconv(ni,no,ks,stride=(2,2)):\n",
    "    return nn.Sequential(*[\n",
    "        nn.ConvTranspose2d(ni,no,kernel_size = ks, stride = stride, padding = (1,1),bias = False),\n",
    "        nn.InstanceNorm2d(no),\n",
    "        nn.LeakyReLU(inplace = True)\n",
    "    ])\n",
    "class resblock(nn.Module):\n",
    "    def __init__(self,fn,ks,shrink=1):\n",
    "        super(resblock,self).__init__()\n",
    "        self.conv1 = conv_layer_ins(fn,fn//shrink,ks)\n",
    "        self.conv2 = conv_layer_ins(fn//shrink,fn,ks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x+self.conv1(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class generative(nn.Module):\n",
    "    def __init__(self,bn,fn=256,k_list = None):\n",
    "        super(generative,self).__init__()\n",
    "        self.bn = bn # block number list\n",
    "        self.fn = fn\n",
    "        self.conv_in = conv_rfpad(3,self.fn//4,7)\n",
    "        \n",
    "        # Down sampling\n",
    "        self.conv_down1 = conv_layer_ins(self.fn//4,self.fn//2,ks=3,stride=2)\n",
    "        self.conv_down2 = conv_layer_ins(self.fn//2,self.fn,ks=3,stride=2)\n",
    "        \n",
    "        for i in range(self.bn):\n",
    "            setattr(self,\"resblock_b%s\"%(i),resblock(self.fn,ks=3))\n",
    "        \n",
    "        # Upsampling using deconv\n",
    "        self.deconv_1 = deconv(self.fn,self.fn//2,ks=3)\n",
    "        self.deconv_2 = deconv(self.fn//2,self.fn//4,ks=3)\n",
    "        self.conv_out = conv_rfpad(self.fn//4,3,ks=7)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv_in(x)\n",
    "        x = self.conv_down2(self.conv_down1(x))\n",
    "        for i in range(self.bn):\n",
    "            x = getattr(self,\"resblock_b%s\"%(i))(x)\n",
    "        x = self.deconv_2(self.deconv_1(x))\n",
    "        return F.tanh(self.conv_out(x))\n",
    "\n",
    "class discriminative(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(discriminative,self).__init__()\n",
    "        self.conv_in = conv_layer_ins(3,64,3,stride=2)\n",
    "        layers = []\n",
    "        self.convs = nn.Sequential(*[\n",
    "            conv_layer_ins(64,128,3,stride=2),\n",
    "            conv_layer_ins(128,256,3,stride=2),\n",
    "            conv_layer_ins(256,512,3,stride=2),\n",
    "            nn.Conv2d(512,1,3,stride=1,padding=1,bias=False),\n",
    "        ])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        bs = x.size()[0]\n",
    "        x = self.convs(self.conv_in(x))\n",
    "        return x.view(bs,-1).mean(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 157\n",
    "WIDTH = 157\n",
    "# DATA  = \"/Users/zhangxiaochen/github/CycleGAN/datasets/summer2winter_yosemite/\"\n",
    "DATA = \"/data/cyclegan/horse2zebra/\"\n",
    "trainA = \"trainA/\"\n",
    "trainB = \"trainB/\"\n",
    "forge = \"/data/forge/\"\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())+\"/\"\n",
    "CLIP_LIMIT = 1e-3\n",
    "BS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import discriminative\n",
    "# from models import generative_chimney2 as generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "CUDA  = cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam,RMSprop\n",
    "class cycle(nn.Module):\n",
    "    def __init__(self,g_fn = 8):\n",
    "        \"\"\"\n",
    "        g_fn: filter numbers for genrative model, a list of int\n",
    "        d_fn: filter numbers for discriminative model, a list of int, downsampling count is len(self.d_fn)-1\n",
    "        \"\"\"\n",
    "        super(cycle,self).__init__()\n",
    "        self.g_fn = g_fn\n",
    "        \n",
    "        # build up generative/discriminative models for both x->y, y->x sides\n",
    "        self.G_x = generative(self.g_fn)\n",
    "        self.D_x = discriminative()\n",
    "        self.G_y = generative(self.g_fn)\n",
    "        self.D_y = discriminative()\n",
    "        if CUDA:\n",
    "            self.G_x = self.G_x.cuda()\n",
    "            self.D_x = self.D_x.cuda()\n",
    "            self.G_y = self.G_y.cuda()\n",
    "            self.D_y = self.D_y.cuda()\n",
    "        \n",
    "        self.opt_Dx = Adam(self.D_x.parameters(),lr=2e-3)\n",
    "        self.opt_Dy = Adam(self.D_y.parameters(),lr=2e-3)\n",
    "        \n",
    "        self.opt_G = Adam(list(self.G_x.parameters())+list(self.G_y.parameters()),lr=2e-3)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        set zerograd for all optimizers\n",
    "        \"\"\"\n",
    "        self.opt_Dx.zero_grad()\n",
    "        self.opt_Dy.zero_grad()\n",
    "        self.opt_G.zero_grad()\n",
    "    \n",
    "    def save_model(self,mdname):\n",
    "        md = getattr(self,mdname)\n",
    "        torch.save(md.state_dict(),\"%s.torch/models/cycle_%s.pkl\"%(HOME,mdname))\n",
    "        \n",
    "    def load_model(self,mdname):\n",
    "        getattr(self,mdname).load_state_dict(torch.load(\"%s.torch/models/cycle_%s.pkl\"%(HOME,mdname)))\n",
    "    \n",
    "    def save(self):\n",
    "        for n in [\"G_x\",\"G_y\",\"D_x\",\"D_y\"]:\n",
    "            self.save_model(n)\n",
    "            \n",
    "    def load(self):\n",
    "        for n in [\"G_x\",\"G_y\",\"D_x\",\"D_y\"]:\n",
    "            self.load_model(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cycle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "class data_cg(Dataset):\n",
    "    def __init__(self,dir_X,dir_Y,iters = 5000):\n",
    "        super(data_cg,self).__init__()\n",
    "        self.iters = iters\n",
    "        self.dir_X = dir_X\n",
    "        self.dir_Y = dir_Y\n",
    "        self.X_list = self.glob_list(self.dir_X)\n",
    "        self.Y_list = self.glob_list(self.dir_Y)\n",
    "        self.X_urls = np.random.choice(self.X_list,self.iters).tolist()\n",
    "        self.Y_urls = np.random.choice(self.Y_list,self.iters).tolist()\n",
    "        self.transform = transforms.Compose([transforms.Resize((HEIGHT,WIDTH)),\n",
    "                                transforms.ToTensor(),\n",
    "                                    # don't apply normalization here\n",
    "#                                 transforms.Normalize([.5,.5,.5],[.5,.5,.5])\n",
    "                               ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.iters\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_url = self.X_urls[idx]\n",
    "        y_url = self.Y_urls[idx]\n",
    "        \n",
    "        X_img = Image.open(x_url).convert(\"RGB\")\n",
    "        Y_img = Image.open(y_url).convert(\"RGB\")\n",
    "        \n",
    "        X = self.transform(X_img)\n",
    "        Y = self.transform(Y_img)\n",
    "        \n",
    "        return X,Y\n",
    "        \n",
    "    def glob_list(self,dir):\n",
    "        if dir[-1]!=\"/\":\n",
    "            dir = dir+\"/\"\n",
    "        dir = dir+\"*\"\n",
    "        return glob(dir)\n",
    "    \n",
    "class data_cg_past(Dataset):\n",
    "    def __init__(self,dir_X,dir_Y,iters = int(5000/2)):\n",
    "        \"\"\"\n",
    "        a dataset to feed past generated samples\n",
    "        \"\"\"\n",
    "        super(data_cg_past,self).__init__()\n",
    "        self.iters = iters\n",
    "        self.dir_X = dir_X\n",
    "        self.dir_Y = dir_Y\n",
    "        self.X_list = self.glob_list(self.dir_X)\n",
    "        self.Y_list = self.glob_list(self.dir_Y)\n",
    "        self.X_urls = np.random.choice(self.X_list,self.iters).tolist()\n",
    "        self.Y_urls = np.random.choice(self.Y_list,self.iters).tolist()\n",
    "        self.transform = transforms.Compose([transforms.Resize((HEIGHT*2,WIDTH)),\n",
    "                                transforms.ToTensor(),\n",
    "\n",
    "                               ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.iters\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_url = self.X_urls[idx]\n",
    "        y_url = self.Y_urls[idx]\n",
    "        \n",
    "        X_img = Image.open(x_url).convert(\"RGB\")\n",
    "        Y_img = Image.open(y_url).convert(\"RGB\")\n",
    "        \n",
    "        X = self.transform(X_img)[:,:HEIGHT,:]\n",
    "        Y = self.transform(Y_img)[:,:HEIGHT,:]\n",
    "        \n",
    "        return X,Y\n",
    "        \n",
    "    def glob_list(self,dir):\n",
    "        if dir[-1]!=\"/\":\n",
    "            dir = dir+\"/\"\n",
    "        dir = dir+\"*\"\n",
    "        return glob(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = data_cg(DATA+\"trainA/\",DATA+\"trainB/\")\n",
    "#ds_past = data_cg_past(forge+\"trainA/\",forge+\"trainB/\")\n",
    "dl = DataLoader(ds,batch_size=2,shuffle=True)\n",
    "\n",
    "gen = iter(dl)\n",
    "\n",
    "a = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl2 = DataLoader(ds_past,batch_size=int(BS/2),shuffle=True)\n",
    "# gen2 = iter(dl2)\n",
    "# a2 = next(gen2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN Loss\n",
    "\n",
    "$\\large L_{GAN}(G,D_{Y},X,Y) = {\\mathbb E}_{y \\tilde{} p_{Data}(y)} [logD_{Y}(y)] +  {\\mathbb E}_{x \\tilde{} p_{Data}(x)} [log(1-D_{Y}(G(x)))]$\n",
    "\n",
    "$\\large L_{GAN}(F,D_{X},Y,X) = {\\mathbb E}_{x \\tilde{} p_{Data}(x)} [logD_{X}(x)] +  {\\mathbb E}_{y \\tilde{} p_{Data}(y)} [log(1-D_{X}(F(y)))]$\n",
    "\n",
    "But in paper's experiment section, it adopted a more practical loss function\n",
    "\n",
    "For GAN loss, $\\large L_{GAN}(G,D_{Y},X,Y)$, we train $\\large G$ to minimize $\\large {\\mathbb E}_{x \\tilde{} p_{Data}(x)} [(D_{Y}(G(x))-1)^{2}]$. This is the loss function we actually coded\n",
    "\n",
    "#### Cycle Consistency Loss\n",
    "\n",
    "$\\large L_{cyc}(G,F) = {\\mathbb E}_{x \\tilde{} p_{Data}(x)}[||F(G(x))-x||_{1}] + {\\mathbb E}_{y \\tilde{} p_{Data}(y)}[||G(F(y))-y||_{1}]$\n",
    "\n",
    "#### Full Objective\n",
    "\n",
    "$\\large L(G,F,D_{X},D_{Y}) =  L_{GAN}(G,D_{Y},X,Y) + L_{GAN}(F,D_{X},Y,X) + \\lambda L_{cyc}(G,F)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from p3self.matchbox import clip_weight\n",
    "\n",
    "os.system(\"mkdir -p /data\")\n",
    "os.system(\"mkdir -p /data/forge\")\n",
    "os.system(\"mkdir -p /data/forge/\"+trainA)\n",
    "os.system(\"mkdir -p /data/forge/\"+trainB)\n",
    "os.system(\"mkdir -p %s.torch\"%(HOME))\n",
    "os.system(\"mkdir -p %s.torch/models\"%(HOME))\n",
    "toIMG = transforms.ToPILImage()\n",
    "\n",
    "class to_images(Dataset):\n",
    "    def __init__(self,tensors,dir_,batch_size=2):\n",
    "        super(to_images,self).__init__()\n",
    "        self.tensors = tensors\n",
    "        self.len = self.tensors.size()[0]\n",
    "        if dir_[-1]!=\"/\":dir_=dir_+\"/\"\n",
    "        self.dir_=dir_\n",
    "        self.batch_size=batch_size\n",
    "        self.dl=DataLoader(self,batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "    def adapt(self,tensors):\n",
    "        self.tensors = tensors\n",
    "        self.len = self.tensors.size()[0]\n",
    "        gen=iter(self.dl)\n",
    "        for i in range(len(self.dl)):\n",
    "            ret = next(gen)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        img = toIMG(self.tensors[idx])\n",
    "#         now=datetime.now().strftime(\"%M_%S_%f\")\n",
    "#         img.save(self.dir_+now+\"_\"+str(idx)+\".png\")\n",
    "        now=datetime.now().strftime(\"%S\")\n",
    "        img.save(self.dir_+now+\"_\"+str(idx)+\".png\")\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveX = to_images(torch.rand(2,3,HEIGHT,WIDTH),forge+trainA)\n",
    "saveY = to_images(torch.rand(2,3,HEIGHT,WIDTH),forge+trainB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_D(D,real,fake):\n",
    "    \"\"\"\n",
    "    loss of discriminator\n",
    "    D: the dsicriminative model\n",
    "    real: image from dataset\n",
    "    fake: the generated image\n",
    "    \"\"\"\n",
    "    dt = torch.cat([real,fake],dim=0)\n",
    "    y_ = D(dt)\n",
    "    y = torch.cat([torch.ones(real.size()[0],1),torch.zeros(fake.size()[0],1)],dim=0)\n",
    "    if CUDA:\n",
    "        y = y.cuda()\n",
    "    return torch.pow(y_-y,2).mean()\n",
    "\n",
    "def train_D(X,Y,past_X = None,past_Y = None,usepast=False):\n",
    "    Y_ = c.G_x(X)\n",
    "    X_ = c.G_y(Y)\n",
    "    \n",
    "    if usepast:\n",
    "        X_fake = torch.cat([X_,past_X],dim = 0)\n",
    "        Y_fake = torch.cat([Y_,past_Y],dim = 0)\n",
    "    else:\n",
    "        X_fake = X_\n",
    "        Y_fake = Y_\n",
    "    \n",
    "    loss_D_x = loss_D(D = c.D_x, real = X, fake = X_fake)\n",
    "    loss_D_y = loss_D(D = c.D_y, real = Y, fake = Y_fake)\n",
    "    \n",
    "    loss_D_x.backward()\n",
    "    loss_D_y.backward()\n",
    "    \n",
    "    c.opt_Dx.step()\n",
    "    c.opt_Dy.step()\n",
    "    \n",
    "    # Save the generated images\n",
    "    if CUDA:\n",
    "        X_,Y_ = X_.cpu(),Y_.cpu()\n",
    "        X_c,Y_c = X.clone().cpu(),Y.clone().cpu()\n",
    "    else:\n",
    "        X_c,Y_c = X.clone(),Y.clone()\n",
    "        \n",
    "    saveX.adapt(torch.cat([X_,Y_c],dim = 2))\n",
    "    saveY.adapt(torch.cat([Y_,X_c],dim = 2))\n",
    "    \n",
    "    return loss_D_x,loss_D_y\n",
    "\n",
    "def train_G(X,Y,lbd = 1e1):\n",
    "    # formula #3 in paper\n",
    "    L_gan_y = torch.pow(c.D_y(c.G_x(X))-1,2).mean()\n",
    "    L_gan_x = torch.pow(c.D_x(c.G_y(Y))-1,2).mean()\n",
    "    L_cycle = torch.abs(c.G_y(c.G_x(X))-X).mean() + torch.abs(c.G_x(c.G_y(Y))-Y).mean()\n",
    "    \n",
    "    Loss = L_gan_y + L_gan_x + lbd*L_cycle\n",
    "    \n",
    "    Loss.backward()\n",
    "    c.opt_G.step()\n",
    "    \n",
    "    return Loss,L_gan_y,L_gan_x,L_cycle\n",
    "    \n",
    "def action(*args,**kwargs):\n",
    "    X,Y = args[0]\n",
    "    ite = kwargs[\"ite\"]\n",
    "    ep = kwargs[\"epoch\"]\n",
    "    if CUDA:\n",
    "        X,Y = X.cuda(),Y.cuda()\n",
    "    c.zero_grad()\n",
    "    if ((ite == 100) and (ep == 0)) or ((ite==0) and (ep>0)):\n",
    "        ds_past = data_cg_past(forge+\"trainA/\",forge+\"trainB/\")\n",
    "        dl_past = DataLoader(ds_past,batch_size=int(BS/2),shuffle=True)\n",
    "        global gen_past\n",
    "        gen_past = iter(dl_past)\n",
    "    \n",
    "    if (ite>100) or (ep > 0):\n",
    "        past_X,past_Y=next(gen_past)\n",
    "        if CUDA:\n",
    "            past_X,past_Y=past_X.cuda(),past_Y.cuda()\n",
    "        loss_D_x,loss_D_y = train_D(X,Y,past_X,past_Y,True)\n",
    "    else:    \n",
    "        loss_D_x,loss_D_y = train_D(X,Y)\n",
    "    \n",
    "#     if ite%3==0:\n",
    "    c.zero_grad()\n",
    "    Loss,L_gan_y,L_gan_x,L_cycle = train_G(X,Y,lbd=1e1)\n",
    "#     else:\n",
    "#         zero=torch.zeros(1)\n",
    "#         Loss = zero\n",
    "#         L_gan_y = zero\n",
    "#         L_gan_x = zero\n",
    "#         L_cycle = zero\n",
    "    # weight clipping\n",
    "#     clip_weight(c.D_x,1e-2)\n",
    "#     clip_weight(c.D_y,1e-2)\n",
    "    \n",
    "    if ite%10==9:\n",
    "        c.save()\n",
    "    return {\"loss_D_x\":loss_D_x.item(),\n",
    "            \"loss_D_y\":loss_D_y.item(),\n",
    "            \"Loss\":Loss.item(),\n",
    "            \"L_gan_y\":L_gan_y.item(),\n",
    "            \"L_gan_x\":L_gan_x.item(),\n",
    "            \"L_cycle\":L_cycle.item(),}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch training framework [matchbox](https://raynardj.github.io/p3self/docs/matchbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p3self.matchbox import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=Trainer(data_cg(DATA+\"trainA/\",DATA+\"trainB/\"),batch_size=BS,print_on=5)\n",
    "trainer.action = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"rm -rf %s\"%(forge+trainA+\"*\"))\n",
    "os.system(\"rm -rf %s\"%(forge+trainB+\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⭐[ep_0_i_54]\tL_cycle\t0.522✨\tL_gan_x\t0.399✨\tL_gan_y\t0.204✨\tLoss\t5.825✨\tloss_D_x\t0.284✨\tloss_D_y\t0.265:   5%|▍         | 58/1250 [02:24<49:31,  2.49s/it] "
     ]
    }
   ],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
